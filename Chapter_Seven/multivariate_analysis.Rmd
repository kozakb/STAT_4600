---
title: "Multivariate Analysis"
author: "Brandon Kozak"
date: "16/11/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE)
```

```{r}
library(tidyverse)
library(BiocManager)
library(here)
library(lattice)
library(ggcorrplot)
library(GGally)
library(ggfortify)
library(factoextra)
library(ggrepel)
library(ade4)
library(pracma)
library(MASS)
```

# Goals of this chapter

* See examples of matrices that come up in the study of biological data.
* Perform dimension reduction to understand correlations between variables
* prepossess data before starting a multivariate analysis
* Build principal components (PC's)\
* Understand how PCA works
* Visualize PCA and learn how to pick the number of PC's

# 7.2 Matrices

Let's look at some data matrices

```{r}

turtles = read.table("../data/PaintedTurtles.txt", header = TRUE)
turtles[1:4, ]

load("../data/athletes.RData")
athletes[1:3, ]

load("../data/Msig3transp.RData")
round(Msig3transp,2)[1:5, 1:6]


data("GlobalPatterns", package = "phyloseq")
GPOTUs = as.matrix(t(phyloseq::otu_table(GlobalPatterns)))
GPOTUs[1:4, 6:13]

library("SummarizedExperiment")
data("airway", package = "airway")
assay(airway)[1:3, 1:4]

metab = t(as.matrix(read.csv("../data/metabolites.csv", row.names = 1)))
metab[1:4, 1:4]
```

Question:

a.) 

* sex, length, width, and height of the turtles
* various measurements of the performance of an athlete for various events.
* gene expression measurements
* taxas
* samples
* mass spectroscopy


b.)

* each turtle
* each athlete
* each gene
* ensemble gene ID
* numerical tag
* samples

c.)

A cell represents the value for a particular row and column pair. 

d.) Sort of surprised they are asking this so far into the book

```{r}
athletes[5,3]
```



## 7.2.1 Low-dimensional data summaries and preparation

Compute the matrix of all correlations between the measurements from the turtles data. What do you notice ?

```{r}
turtles_num = turtles %>% mutate(sex=ifelse(sex=="f",0,1))
turt_cor = cor(turtles_num)

ggcorrplot(turt_cor, type = "lower", lab = TRUE,
           ggtheme = ggplot2::theme_gray,
           colors = c("#6D9EC1", "white", "#E46726"))
```

Strong positive correlation in width and height, length and width and, length and height.

Moderate negative correlation in sex and the other 3 measurements (note 0 is female and 1 is male).





```{r}
ggpairs(turtles_num, axisLabels = "none")

```


```{r}

library("pheatmap")
pheatmap(cor(athletes), cell.width = 10, cell.height = 10)

```



## 7.2.2 Preprocessing the data

```{r}
# normal 
apply(turtles[,-1], 2, sd)
apply(turtles[,-1], 2, mean)

# scaled
scaledTurtles = scale(turtles[, -1])

apply(scaledTurtles, 2, mean)
apply(scaledTurtles, 2, sd)

data.frame(scaledTurtles, sex = turtles[, 1]) %>%
  ggplot(aes(x = width, y = height, group = sex)) +
    geom_point(aes(color = sex)) + coord_fixed()
```


# 7.3 Dimension reduction


## 7.3.1 Lower-dimensional projections

here is one example of going from 2d to 1d

```{r}
athletes_scale = data.frame(scale(athletes))

ath_gg = ggplot(athletes_scale, aes(x = weight, y = disc)) +
  geom_point(size = 2, shape = 21)

ath_gg + geom_point(aes(y = 0), colour = "red") +
  geom_segment(aes(xend = weight, yend = 0), linetype = "dashed")
```

the variance is just the variance of the weight, scaled or not.

```{r}
var(scale(athletes$weight))
```

We can also project to the y axis
```{r}
ath_gg = ggplot(athletes_scale, aes(x = weight, y = disc)) +
  geom_point(size = 2, shape = 21)

ath_gg + geom_point(aes(x = 0), colour = "blue") +
  geom_segment(aes(yend = disc, xend = 0), linetype = "dashed")
```

the variance is just the variance of the disc, scaled or not.

```{r}
var((athletes$disc))
```


## 7.3.2 How do we summarize two-dimensional data by a line?

Of course doing what we just did loses all of the information about the other variables.

One idea that we can use is regression, it too will reduce our 2d problem to a 1d line, but still keep info about both variables.

```{r}

reg1 = lm(disc ~ weight, data = athletes)
a1 = reg1$coefficients[1] # intercept
b1 = reg1$coefficients[2] # slope
pline1 = ath_gg + geom_abline(intercept = a1, slope = b1,
    col = "blue", lwd = 1.5)
pline1 + geom_segment(aes(xend = weight, yend = reg1$fitted),
    colour = "red", arrow = arrow(length = unit(0.15, "cm")))
```


```{r}
var(athletes$weight) + var(reg1$fitted)
```

we see the variance is larger

We can also minimize the distance in both the x and y direction, we will see this is a big idea in PCA

```{r}
xy = cbind(scale(athletes$disc), scale(athletes$weight))
svda = svd(xy)
pc = xy %*% svda$v[, 1] %*% t(svda$v[, 1])
bp = svda$v[2, 1] / svda$v[1, 1]
ap = mean(pc[, 2]) - bp * mean(pc[, 1])
ath_gg + geom_segment(xend = pc[, 1], yend = pc[, 2]) +
  geom_abline(intercept = ap, slope = bp, col = "purple", lwd = 1.5)
```

# 7.4 The new linear combinations

A principal component is simply a linear combination the variables that we wish to reduce in dimension.

The coefficients that make us this linear combination are called **loading**.

Once we apply the linear combination to the data, we get the principal component.

# 7.5 The PCA workflow

The big picture idea for PCA is to find the axis (variable) that shows the most variability, then iterate to find the next and so forth. 

However, we do not need to iterate, but rather we can decompose our data using Singular Value Decomposition. 

Of course there is alot of choices that we have to make. For example, do we use scaled data, and how many PC's do we use?

# 7.6 The inner workings of PCA: rank reduction

SVD takes a matrix A and decomposes as follows $A = U\Sigma V^T$ where:

* A is a $m\times n$ matrix
* U is a $m \times m$ orthogonal matrix
* $\Sigma$ is a $m \times n$ diagonal matrix
* V is a $n \times n$ orthogonal matrix

Note that U and V contain the singular vectors, and $\Sigma$ contains the singular values.

## Case of rank one

This almost never happens

## All other cases

One thing that gets me here, is that we focus on using SVD but never look at the other approach where we find the eigen values and vectors of the covariance matrix.

```{r}

Xtwo = matrix(c(12.5, 35.0, 25.0, 25, 9, 14, 26, 18, 16, 21, 49, 32,
                
       18, 28, 52, 36, 18, 10.5, 64.5, 36), ncol = 4, byrow = TRUE)
USV = svd(Xtwo)

USV$d[1] %*% USV$u[,1]

```

# 7.7 Plotting the observations in the principal plane

What part of the output of the svd functions leads us to the first PC coefficients, also known as the PC loadings ?

This would be the "v" matrix, in particular the first column gives us the first set of loadings.

Let's get right into things by performing PCA on the athletes data

```{r}
pca_ath = prcomp(athletes,scale=T)

screeplot(pca_ath)

autoplot(pca_ath,data=athletes,colour="m100",loadings = TRUE,
         loadings.colour = 'green',loadings.label = TRUE, loadings.label.size = 3)

```



So, what does this tell us?

First, the screeplot tells us that the first two PC's explain about 50% of the variability between all covariates.

If we focus in on the first two PC's then we can plot a bi-plot. This shows us that the first PC is mainly driven by events that require running or jumping, where as the second PC is mostly driven by events that require throwing.

Also, note the negative weights on the race variable (m100,m110,etc...). This is the case since the original measure was time, and being "good" at the race typically means having a shorter time. This negative correlation (greater than 90 degree angle) with the jumping variables was also seen with the heat map we produced above.

## 7.7.1 PCA of the turtles data

```{r}
pca_tur = prcomp(turtles[,-1],scale=T)
pcaturtles = princomp(scaledTurtles)

screeplot(pca_tur)

autoplot(pca_tur,data=turtles,colour="sex",loadings = TRUE,
         loadings.colour = 'black',loadings.label = TRUE, loadings.label.size = 3) +
  scale_color_manual(values = c("orange", "skyblue4"))

fviz_pca_biplot(pca_tur, label = "var", habillage = turtles[, 1]) +
  ggtitle("")
```

Why do width and height form a 90 degree angle? I thought this meant that they where uncorrelated, but clearly they are not.

Also we see that female turtles tend to gather on the positive side of the x-axis, while males tend to gather on the negative side of the x-axis. We conclude that female turtles are larger in the data set.


## 7.7.2 A complete analysis: the decathlon athletes

I sort of jumped the gun here and already did PCA on the athletes data, but we can explore a few more things.



What transformations of the variables induce the best athletic performances to vary in the same direction, i.e. be mostly positively correlated?

Since we already know that shorter times are better, we can simplify negate all time measures to have all of theses variables be positively correlated.

```{r}
athletes_neg = athletes
athletes_neg[, c(1, 5, 6, 10)] = -athletes_neg[, c(1, 5, 6, 10)]

pca_ath_neg = prcomp(athletes_neg,scale=T)


screeplot(pca_ath_neg)

autoplot(pca_ath_neg,data=athletes,colour="m100",loadings = TRUE,
         loadings.colour = 'green',loadings.label = TRUE, loadings.label.size = 3)
```

```{r}
data("olympic", package = "ade4")

data = tibble(id=1:33,score=olympic$score,pc1=pca_ath$x[,1])

data %>% ggplot(aes(x=score,y=pc1,color=id)) + geom_point() + 
  scale_colour_gradient(low="cyan3", high="springgreen4") +
  geom_text_repel(aes(label = id),
                  box.padding   = 0.1, 
                  point.padding = 0.1,
                  segment.color = 'grey50')
```


So we see that a higher value for PC1 indicates a better score. I believe that in the book the function they use to obtain the PC's reverse the signs of the loadings. In any case, this makes sense since PC1 captures most of the information about the events, and a larger PC1 only happens when the lower times and higher distances are obtained in a event.

## 7.7.3 How to choose k, the number of dimensions ?

Two options:

* Pick some constant threshold $c$, the number of PC's we keep is equal to $p$ where $p$ is the PC who's cumulative explained variance is equal to or greater than $c$
* Look at the scree plot and pick a point where a sharp drop occurs. Of course, this may not always be obvious.


# 7.8 PCA as an exploratory tool: using extra information

```{r}
pcaMsig3 = dudi.pca(Msig3transp, center = TRUE, scale = TRUE,
                    scannf = FALSE, nf = 4)
fviz_screeplot(pcaMsig3) + ggtitle("")


ids = rownames(Msig3transp)
celltypes = factor(substr(ids, 7, 9))
status = factor(substr(ids, 1, 3))
table(celltypes)


cbind(pcaMsig3$li, tibble(Cluster = celltypes, sample = ids)) %>%
ggplot(aes(x = Axis1, y = Axis2)) +
  geom_point(aes(color = Cluster), size = 5) +
  geom_hline(yintercept = 0, linetype = 2) +
  geom_vline(xintercept = 0, linetype = 2) +
  scale_color_discrete(name = "Cluster") + coord_fixed()

```

Not sure what they are trying to show us here.

## 7.8.2 Biplots and scaling


```{r}
library("pheatmap")
load("../data/wine.RData")
load("../data/wineClass.RData")
wine[1:2, 1:7]

pheatmap(1 - cor(wine), treeheight_row = 0.2)

winePCAd = dudi.pca(wine, scannf=FALSE)
table(wine.class)

fviz_pca_biplot(winePCAd, geom = "point", habillage = wine.class,
   col.var = "violet", addEllipses = TRUE, ellipse.level = 0.69) +
   ggtitle("") + coord_fixed()
```

From what I gather, PC's become hard to interpret when we include a lot of variables, and lack domain knowledge.

We can, however gather knowledge about groups in our data this way. In the above we can determine that barbera wines have more malic acid on average. 

## 7.8.3 An example of weighted PCA

Used when sample sizes vary by quite a bit.

```{r}

data("x", package = "Hiiragi2013")
xwt = x[, x$genotype == "WT"]
sel = order(rowVars(Biobase::exprs(xwt)), decreasing = TRUE)[1:100]
xwt = xwt[sel, ]
tab = table(xwt$sampleGroup)
tab


xwt$weight = 1 / as.numeric(tab[xwt$sampleGroup])
pcaMouse = dudi.pca(as.data.frame(t(Biobase::exprs(xwt))),
  row.w = xwt$weight,
  center = TRUE, scale = TRUE, nf = 2, scannf = FALSE)
fviz_eig(pcaMouse) + ggtitle("")

fviz_pca_ind(pcaMouse, geom = "point", col.ind = xwt$sampleGroup) +
  ggtitle("") + coord_fixed()
```



# 7.11 Exercises


## 7.1

a.) I do not believe so, since eigen vectors are also not unique.

b.)

```{r}
v1 = seq(2, 30, by = 2)
v2 = seq(3, 12, by = 3)
A = v1 %*% t(v2)
```

We must multiply by the transpose since we need the dimensions to match.

c.)

```{r}
Materr = matrix(rnorm(60,1),nrow=15,ncol=4)
A = A+Materr
```

d.)

```{r}
levelplot(A)
```

repeat for rank 2 matrix.)

Not really sure n=how to generate, so just found a 2 rank example

```{r}
A = matrix(c(2,3,4,9,3,4,5,10,4,5,6,11,5,6,7,12), ncol=4)

levelplot(A)
```

## 7.2

7.2a Create a matrix of highly correlated bivariate data such as that shown in Figure 7.35.
Hint: Use the function mvrnorm.  

```{r exercise7.2, eval=TRUE}
#our parameters
mu = c(1,3)
sigma = matrix(c(1,.98,.98,1),ncol=2)

# the data
bi_data = data.frame(mvrnorm(100, mu,sigma))


ggplot(data.frame(bi_data),aes(x=X1,y=X2)) + geom_point()
```

Check the rank of the matrix by looking at its singular values.  

```{r rankcheck, eval=TRUE}
rref(svd(scale(bi_data))$v)
```

Looks like a full (2) rank matrix to me

b.) 

```{r exercise7.2b, eval=TRUE}
pca_bi = prcomp(bi_data)

autoplot(pca_bi,loadings = TRUE,
         loadings.colour = 'orange',loadings.label = TRUE)
```


## 7.3

```{r}
mu1 = 1; mu2 = 2; s1=2.5; s2=0.8; rho=0.9;
sigma = matrix(c(s1^2, s1*s2*rho, s1*s2*rho, s2^2),2)
library("MASS")
sim2d = data.frame(mvrnorm(50, mu = c(mu1,mu2), Sigma = sigma))
svd(scale(sim2d))$d
svd(scale(sim2d))$v[,1]
ggplot(data.frame(sim2d),aes(x=X1,y=X2)) +
    geom_point()
respc=princomp(sim2d)
dfpc = data.frame(pc1=respc$scores[,1],
pc2 = respc$scores[,2])
 ggplot(dfpc,aes(x=pc1,y=pc2)) +
   geom_point() + coord_fixed(2)

```

It's elongated because the 1st PC is on the x-axis and explains the most variability in our covariates.

Not fixing the coords will make it appear that the 1st PC doesn't explain as much variability as it actually does.

## 7.4


```{r}

data("x", package = "Hiiragi2013")
xwt = x[, x$genotype == "WT"]
sel = order(rowVars(Biobase::exprs(xwt)), decreasing = TRUE)[1:100]
xwt = xwt[sel, ]


pcaMouse = prcomp(as.data.frame(t(Biobase::exprs(xwt))))


screeplot(pcaMouse)

autoplot(pcaMouse,loadings = TRUE,
         loadings.colour = 'blue', loadings.label.size = 3)
```